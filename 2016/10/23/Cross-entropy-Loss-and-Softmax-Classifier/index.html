<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Cross-entropy Loss and Softmax Classifier | Random Thoughts on Deep Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Cross-entropy loss is a commonly adopted loss function for classification task, which involves the use of Softmax classifier.
Technically, the cross-entropy loss is defined as the cross-entropy betwee">
<meta property="og:type" content="article">
<meta property="og:title" content="Cross-entropy Loss and Softmax Classifier">
<meta property="og:url" content="http://yoursite.com/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/index.html">
<meta property="og:site_name" content="Random Thoughts on Deep Learning">
<meta property="og:description" content="Cross-entropy loss is a commonly adopted loss function for classification task, which involves the use of Softmax classifier.
Technically, the cross-entropy loss is defined as the cross-entropy betwee">
<meta property="og:updated_time" content="2016-10-23T08:57:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cross-entropy Loss and Softmax Classifier">
<meta name="twitter:description" content="Cross-entropy loss is a commonly adopted loss function for classification task, which involves the use of Softmax classifier.
Technically, the cross-entropy loss is defined as the cross-entropy betwee">
  
    <link rel="alternate" href="/atom.xml" title="Random Thoughts on Deep Learning" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Random Thoughts on Deep Learning</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Cross-entropy-Loss-and-Softmax-Classifier" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/" class="article-date">
  <time datetime="2016-10-23T08:06:30.000Z" itemprop="datePublished">2016-10-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Cross-entropy Loss and Softmax Classifier
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Cross-entropy loss is a commonly adopted loss function for classification task, which involves the use of Softmax classifier.</p>
<p>Technically, the cross-entropy loss is defined as the cross-entropy between a true distribution p and an estimated distribution q <a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" rel="external">1</a>:</p>
<span>$$\begin{aligned}
H(p,q)=-\sum_{x}p(x)log(q(x)) \\
\end{aligned}$$</span><!-- Has MathJax -->
<p>Hence, the objective of Softmax classifier is to minimize the cross-entropy between the estimated distribution and the true distribution. For instance, the loss of a training example can be calculated according to the following equation <a href="http://cs224d.stanford.edu/lecture_notes/notes2.pdf" target="_blank" rel="external">2</a>:</p>
<span>$$\begin{aligned}
-\sum_{j=1}^{C} y_{j} log(q(y_j=1|x))=-\sum_{j=1}^{C} y_{j} log (\frac{exp(W_j x)}{\sum_{c=1}^{C} exp(W_cx)}) \\
\end{aligned}$$</span><!-- Has MathJax -->
<p>where,<br><span>$$\begin{aligned}
q(y_j=1|x)=\frac{exp(W_j x)}{\sum_{c=1}^{C} exp(W_cx)}\\
\end{aligned}$$</span><!-- Has MathJax --><br>p(x) has the following distribution p=[0,…,1,…,0]. y_j=1 is 1 only if sample x belongs to the correct class. Therefore, this equation can be simplified as <a href="http://cs224d.stanford.edu/lecture_notes/notes2.pdf" target="_blank" rel="external">2</a>,<br><span>$$\begin{aligned}
-log(\frac{exp(W_{k}x)}{\sum_{c=1}^{C}exp(W_c x)})\\
\end{aligned}$$</span><!-- Has MathJax --><br>This is basically the minimization of negative log likelihood. In practice, Softmax classifier works in this way. First, it applies the Softmax function to normalize the scores to obtain probabilities. Then, it computes the negative log likelihood of these probabilities. And finally, the summation of these negative log likelihoods is minimized. Oftentimes, people also use Softmax loss and cross-entropy loss interchangeably. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/" data-id="ciumehztt0001zds6uibdq3se" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/10/08/A-toy-example-with-DL/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">A toy example with DL</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/">Cross-entropy Loss and Softmax Classifier</a>
          </li>
        
          <li>
            <a href="/2016/10/08/A-toy-example-with-DL/">A toy example with DL</a>
          </li>
        
          <li>
            <a href="/2016/10/08/Softmax-Loss-Derivative/">Softmax-Loss Derivative</a>
          </li>
        
          <li>
            <a href="/2016/10/02/Transfer-Learning-Tips/">Transfer_Learning_Tips</a>
          </li>
        
          <li>
            <a href="/2016/10/02/Deep-Learning-Resources/">Deep_Learning_Resources</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Cunjian Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>