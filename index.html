<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Random Thoughts on Deep Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Random Thoughts on Deep Learning">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Random Thoughts on Deep Learning">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Random Thoughts on Deep Learning">
<meta name="twitter:description">
  
    <link rel="alternate" href="/atom.xml" title="Random Thoughts on Deep Learning" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Random Thoughts on Deep Learning</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Faster-RCNN-Installation-Guide" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/01/Faster-RCNN-Installation-Guide/" class="article-date">
  <time datetime="2016-11-01T14:41:47.000Z" itemprop="datePublished">2016-11-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/01/Faster-RCNN-Installation-Guide/">Faster RCNN Installation Guide</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Here is a guide to install py-faster-rcnn, in addition to the information provided on the official website.</p>
<p><a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/py-faster-rcnn</a></p>
<ol>
<li>Clone the Faster R-CNN repository</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Make sure to clone with --recursive</span><br><span class="line">git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git</span><br></pre></td></tr></table></figure>
<ol>
<li>Build the Cython modules (FRCN_ROOT is the directory where you store the downloaded library)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $FRCN_ROOT/lib</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<p>Modify the following if you want to compile without GPU<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">???</span><br><span class="line">#CUDA = locate_cuda()</span><br><span class="line"></span><br><span class="line">???</span><br><span class="line">#            self.set_executable(&apos;compiler_so&apos;, CUDA[&apos;nvcc&apos;])</span><br><span class="line">???</span><br><span class="line">#    Extension(&apos;nms.gpu_nms&apos;,</span><br><span class="line">#        [&apos;nms/nms_kernel.cu&apos;, &apos;nms/gpu_nms.pyx&apos;],</span><br><span class="line">#        library_dirs=[CUDA[&apos;lib64&apos;]],</span><br><span class="line">#        libraries=[&apos;cudart&apos;],</span><br><span class="line">#        language=&apos;c++&apos;,</span><br><span class="line">#        runtime_library_dirs=[CUDA[&apos;lib64&apos;]],</span><br><span class="line">#        # this syntax is specific to this build system</span><br><span class="line">#        # we&apos;re only going to use certain compiler args with nvcc and not with</span><br><span class="line">#        # gcc the implementation of this trick is in customize_compiler() below</span><br><span class="line">#        extra_compile_args=&#123;&apos;gcc&apos;: [&quot;-Wno-unused-function&quot;],</span><br><span class="line">#                            &apos;nvcc&apos;: [&apos;-arch=sm_35&apos;,</span><br><span class="line">#                                     &apos;--ptxas-options=-v&apos;,</span><br><span class="line">#                                     &apos;-c&apos;,</span><br><span class="line">#                                     &apos;--compiler-options&apos;,</span><br><span class="line">#                                     &quot;&apos;-fPIC&apos;&quot;]&#125;,</span><br><span class="line">#        include_dirs = [numpy_include, CUDA[&apos;include&apos;]]</span><br><span class="line">#    ),</span><br><span class="line">???</span><br></pre></td></tr></table></figure></p>
<ol>
<li>Build Caffe and pycaffe<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $FRCN_ROOT/caffe-fast-rcnn</span><br><span class="line">make -j8 &amp;&amp; make pycaffe</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Use the following makefile.config</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## Refer to http://caffe.berkeleyvision.org/installation.html</span><br><span class="line"># Contributions simplifying and improving our build system are welcome!</span><br><span class="line"></span><br><span class="line"># cuDNN acceleration switch (uncomment to build with cuDNN).</span><br><span class="line"># USE_CUDNN := 1</span><br><span class="line"></span><br><span class="line"># CPU-only switch (uncomment to build without GPU support).</span><br><span class="line">CPU_ONLY := 1</span><br><span class="line"></span><br><span class="line"># uncomment to disable IO dependencies and corresponding data layers</span><br><span class="line"># USE_OPENCV := 0</span><br><span class="line"># USE_LEVELDB := 0</span><br><span class="line"># USE_LMDB := 0</span><br><span class="line"></span><br><span class="line"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span><br><span class="line">#    You should not set this flag if you will be reading LMDBs with any</span><br><span class="line">#    possibility of simultaneous read and write</span><br><span class="line"># ALLOW_LMDB_NOLOCK := 1</span><br><span class="line"></span><br><span class="line"># Uncomment if you&apos;re using OpenCV 3</span><br><span class="line"># OPENCV_VERSION := 3</span><br><span class="line"></span><br><span class="line"># To customize your choice of compiler, uncomment and set the following.</span><br><span class="line"># N.B. the default for Linux is g++ and the default for OSX is clang++</span><br><span class="line"># CUSTOM_CXX := g++</span><br><span class="line"></span><br><span class="line"># CUDA directory contains bin/ and lib/ directories that we need.</span><br><span class="line"># CUDA_DIR := /usr/local/cuda</span><br><span class="line"># On Ubuntu 14.04, if cuda tools are installed via</span><br><span class="line"># &quot;sudo apt-get install nvidia-cuda-toolkit&quot; then use this instead:</span><br><span class="line"># CUDA_DIR := /usr</span><br><span class="line"></span><br><span class="line"># CUDA architecture setting: going with all of them.</span><br><span class="line"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span><br><span class="line">#CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</span><br><span class="line">#        -gencode arch=compute_20,code=sm_21 \</span><br><span class="line">#        -gencode arch=compute_30,code=sm_30 \</span><br><span class="line">#        -gencode arch=compute_35,code=sm_35 \</span><br><span class="line">#        -gencode arch=compute_50,code=sm_50 \</span><br><span class="line">#        -gencode arch=compute_50,code=compute_50</span><br><span class="line"></span><br><span class="line"># BLAS choice:</span><br><span class="line"># atlas for ATLAS (default)</span><br><span class="line"># mkl for MKL</span><br><span class="line"># open for OpenBlas</span><br><span class="line">BLAS := open</span><br><span class="line"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span><br><span class="line"># Leave commented to accept the defaults for your choice of BLAS</span><br><span class="line"># (which should work)!</span><br><span class="line">BLAS_INCLUDE := /usr/include/atlas-x86_64-base</span><br><span class="line">BLAS_LIB := /usr/lib64/atlas</span><br><span class="line"></span><br><span class="line"># Homebrew puts openblas in a directory that is not on the standard search path</span><br><span class="line"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span><br><span class="line"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span><br><span class="line"></span><br><span class="line"># This is required only if you will compile the matlab interface.</span><br><span class="line"># MATLAB directory should contain the mex binary in /bin.</span><br><span class="line"># MATLAB_DIR := /usr/local</span><br><span class="line"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span><br><span class="line"></span><br><span class="line"># NOTE: this is required only if you will compile the python interface.</span><br><span class="line"># We need to be able to find Python.h and numpy/arrayobject.h.</span><br><span class="line">PYTHON_INCLUDE := /usr/include/python2.7 \</span><br><span class="line">                  /usr/lib64/python2.7/site-packages/numpy/core/include</span><br><span class="line"># Anaconda Python distribution is quite popular. Include path:</span><br><span class="line"># Verify anaconda location, sometimes it&apos;s in root.</span><br><span class="line"> ANACONDA_HOME := $(HOME)/anaconda2</span><br><span class="line"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span><br><span class="line">         $(ANACONDA_HOME)/include/python2.7 \</span><br><span class="line">         $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \</span><br><span class="line"></span><br><span class="line"># Uncomment to use Python 3 (default is Python 2)</span><br><span class="line"># PYTHON_LIBRARIES := boost_python3 python3.5m</span><br><span class="line"># PYTHON_INCLUDE := /usr/include/python3.5m \</span><br><span class="line">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span><br><span class="line"></span><br><span class="line"># We need to be able to find libpythonX.X.so or .dylib.</span><br><span class="line">PYTHON_LIB := /usr/lib64</span><br><span class="line">PYTHON_LIB := $(ANACONDA_HOME)/lib</span><br><span class="line"></span><br><span class="line"># Homebrew installs numpy in a non standard path (keg only)</span><br><span class="line"># PYTHON_INCLUDE += $(dir $(shell python -c &apos;import numpy.core; print(numpy.core.__file__)&apos;))/include</span><br><span class="line"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span><br><span class="line"></span><br><span class="line"># Uncomment to support layers written in Python (will link against Python libs)</span><br><span class="line">WITH_PYTHON_LAYER := 1</span><br><span class="line"></span><br><span class="line"># Whatever else you find you need goes here.</span><br><span class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/include</span><br><span class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/lib64 /usr/lib</span><br><span class="line"></span><br><span class="line"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span><br><span class="line"># INCLUDE_DIRS += $(shell brew --prefix)/include</span><br><span class="line"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span><br><span class="line"></span><br><span class="line"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span><br><span class="line"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span><br><span class="line"># USE_PKG_CONFIG := 1</span><br><span class="line"></span><br><span class="line">BUILD_DIR := build</span><br><span class="line">DISTRIBUTE_DIR := distribute</span><br><span class="line"></span><br><span class="line"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span><br><span class="line"># DEBUG := 1</span><br><span class="line"></span><br><span class="line"># The ID of the GPU that &apos;make runtest&apos; will use to run unit tests.</span><br><span class="line"># TEST_GPUID := 0</span><br><span class="line"></span><br><span class="line"># enable pretty build (comment to see full commands)</span><br><span class="line">Q ?= @</span><br></pre></td></tr></table></figure>
<ol>
<li><p>In case you encounter libopencv_highgui.so error, use the following to fix the issue and then compile again.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/x86_64-linux-gnu</span><br><span class="line">sudo ln -s ~/anaconda2/lib/libpng16.so.16 libpng16.so.16</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download pre-computed Faster R-CNN detectors</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $FRCN_ROOT</span><br><span class="line">./data/scripts/fetch_faster_rcnn_models.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run the demo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $FRCN_ROOT</span><br><span class="line">./tools/demo.py</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>For CPU, you need to modify /lib/fast-rcnn/nms_wrapper.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def nms (dets,thresh, force_cpu=True)</span><br><span class="line">#from nms.gpu_nms import gpu_nms</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/demo.py --cpu</span><br></pre></td></tr></table></figure>
<p>Now see the results here.</p>
<h2 id="Demo-results-by-Faster-RCNN"><a href="#Demo-results-by-Faster-RCNN" class="headerlink" title="Demo results by Faster-RCNN"></a>Demo results by Faster-RCNN</h2><div id="image-table"><br>    <table><br>        <tr><br>            <td style="padding:5px"><br>                <img src="img/faster-rcnn/figure_1.png" height="200" width="250"><br>              </td><br>            <td style="padding:5px"><br>                <img src="img/faster-rcnn/figure_3.png" height="200" width="250"><br>             </td><br>            <td style="padding:5px"><br>                <img src="img/faster-rcnn/figure_5.png" height="200" width="250"><br>             </td><br>        </tr><br>    </table><br></div>







      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/11/01/Faster-RCNN-Installation-Guide/" data-id="ciuzmttis0003ohs60nrtvo9a" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Torch-Installation-Guide" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/30/Torch-Installation-Guide/" class="article-date">
  <time datetime="2016-10-30T08:10:53.000Z" itemprop="datePublished">2016-10-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/30/Torch-Installation-Guide/">Torch Installation Guide</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>The following is a guideline to install Torch on Mac OSX Yosemite.</p>
<p>First, follow the ins</p>
<p>tructions given on the official website.</p>
<h1 id="in-a-terminal-run-the-commands-WITHOUT-sudo"><a href="#in-a-terminal-run-the-commands-WITHOUT-sudo" class="headerlink" title="in a terminal, run the commands WITHOUT sudo"></a>in a terminal, run the commands WITHOUT sudo</h1><p>git clone <a href="https://github.com/torch/distro.git" target="_blank" rel="external">https://github.com/torch/distro.git</a> ~/torch –recursive<br>cd ~/torch; bash install-deps;<br>./install.sh</p>
<p>Then, we need to add the torch into the PATH variable. </p>
<p>touch ~/.bash_profile; open ~/.bash_profile</p>
<p>Once the file is open, add the following two lines.</p>
<h1 id="TORCH"><a href="#TORCH" class="headerlink" title="TORCH"></a>TORCH</h1><p>export PATH=$PATH:/Users/<your user="" name="">/torch/install/bin</your></p>
<p>You need to replace “your user name” with the name you have on your computer and check the torch path to make sure it is correct. </p>
<p>After that, you should be able to use “th” command. </p>
<p>Reference:<br><a href="http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/" target="_blank" rel="external">http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/30/Torch-Installation-Guide/" data-id="ciuzmttjb0008ohs6z3ln8w9l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Cross-entropy-Loss-and-Softmax-Classifier" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/" class="article-date">
  <time datetime="2016-10-23T08:06:30.000Z" itemprop="datePublished">2016-10-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/">Cross-entropy Loss and Softmax Classifier</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Cross-entropy loss is a commonly adopted loss function for classification task, which involves the use of Softmax classifier.</p>
<p>Technically, the cross-entropy loss is defined as the cross-entropy between a true distribution p and an estimated distribution q <a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" rel="external">1</a>:</p>
<span>$$\begin{aligned}
H(p,q)=-\sum_{x}p(x)log(q(x)) \\
\end{aligned}$$</span><!-- Has MathJax -->
<p>Hence, the objective of Softmax classifier is to minimize the cross-entropy between the estimated distribution and the true distribution. For instance, the loss of a training example can be calculated according to the following equation <a href="http://cs224d.stanford.edu/lecture_notes/notes2.pdf" target="_blank" rel="external">2</a>:</p>
<span>$$\begin{aligned}
-\sum_{j=1}^{C} y_{j} log(q(y_j=1|x))=-\sum_{j=1}^{C} y_{j} log (\frac{exp(W_j x)}{\sum_{c=1}^{C} exp(W_cx)}) \\
\end{aligned}$$</span><!-- Has MathJax -->
<p>where,<br><span>$$\begin{aligned}
q(y_j=1|x)=\frac{exp(W_j x)}{\sum_{c=1}^{C} exp(W_cx)}\\
\end{aligned}$$</span><!-- Has MathJax --><br>p(x) has the following distribution p=[0,…,1,…,0]. y_j=1 is 1 only if sample x belongs to the correct class. Therefore, this equation can be simplified as <a href="http://cs224d.stanford.edu/lecture_notes/notes2.pdf" target="_blank" rel="external">2</a>,<br><span>$$\begin{aligned}
-log(\frac{exp(W_{k}x)}{\sum_{c=1}^{C}exp(W_c x)})\\
\end{aligned}$$</span><!-- Has MathJax --><br>This is basically the minimization of negative log likelihood. In practice, Softmax classifier works in this way. First, it applies the Softmax function to normalize the scores to obtain probabilities. Then, it computes the negative log likelihood of these probabilities. And finally, the summation of these negative log likelihoods is minimized. Oftentimes, people also use Softmax loss and cross-entropy loss interchangeably. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/" data-id="ciuzmttj80007ohs65skhl832" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A-toy-example-with-DL" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/08/A-toy-example-with-DL/" class="article-date">
  <time datetime="2016-10-08T14:46:41.000Z" itemprop="datePublished">2016-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/08/A-toy-example-with-DL/">A toy example with DL</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>In the course of cs231n, there is a toy example which conveys the idea of how back propagation works. I found this one to be particularly interesting. </p>
<h2 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h2><p>import numpy as np<br>import matplotlib as plt</p>
<p>N = 100 # number of points per class<br>D = 2 # dimensionality<br>K = 3 # number of classes<br>X = np.zeros((N<em>K,D)) # data matrix (each row = single example)<br>y = np.zeros(N</em>K, dtype=’uint8’) # class labels<br>for j in xrange(K):<br>  ix = range(N<em>j,N</em>(j+1))<br>  r = np.linspace(0.0,1,N) # radius<br>  t = np.linspace(j<em>4,(j+1)</em>4,N) + np.random.randn(N)<em>0.2 # theta<br>  X[ix] = np.c_[r</em>np.sin(t), r*np.cos(t)]<br>  y[ix] = j</p>
<p>h = 100 # size of hidden layer<br>W = 0.01 <em> np.random.randn(D,h)<br>b = np.zeros((1,h))<br>W2 = 0.01 </em> np.random.randn(h,K)<br>b2 = np.zeros((1,K))</p>
<p>step_size = 1e-0<br>reg = 1e-3 # regularization strength</p>
<p>% gradient descent loop<br>num_examples = X.shape[0]<br>for i in xrange(10000):</p>
<p>  % evaluate class scores, [N x K]<br>  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation<br>  scores = np.dot(hidden_layer, W2) + b2</p>
<p>  % compute the class probabilities<br>  exp_scores = np.exp(scores)<br>  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]</p>
<p>  % compute the loss: average cross-entropy loss and regularization<br>  corect_logprobs = -np.log(probs[range(num_examples),y])<br>  data_loss = np.sum(corect_logprobs)/num_examples<br>  reg_loss = 0.5<em>reg</em>np.sum(W<em>W) + 0.5</em>reg<em>np.sum(W2</em>W2)<br>  loss = data_loss + reg_loss<br>  if i % 1000 == 0:<br>    print “iteration %d: loss %f” % (i, loss)</p>
<p>  % compute the gradient on scores<br>  dscores = probs<br>  dscores[range(num_examples),y] -= 1<br>  dscores /= num_examples</p>
<p>  % backpropate the gradient to the parameters<br>  % first backprop into parameters W2 and b2<br>  dW2 = np.dot(hidden_layer.T, dscores)<br>  db2 = np.sum(dscores, axis=0, keepdims=True)<br>  % next backprop into hidden layer<br>  dhidden = np.dot(dscores, W2.T)<br>  % backprop the ReLU non-linearity<br>  dhidden[hidden_layer &lt;= 0] = 0<br>  % finally into W,b<br>  dW = np.dot(X.T, dhidden)<br>  db = np.sum(dhidden, axis=0, keepdims=True)</p>
<p>  % add regularization gradient contribution<br>  dW2 += reg <em> W2<br>  dW += reg </em> W</p>
<p>  % perform a parameter update<br>  W += -step_size <em> dW<br>  b += -step_size </em> db<br>  W2 += -step_size <em> dW2<br>  b2 += -step_size </em> db2</p>
<p>% evaluate training set accuracy<br>hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation<br>scores = np.dot(hidden_layer, W2) + b2<br>predicted_class = np.argmax(scores, axis=1)<br>print ‘training accuracy: %.2f’ % (np.mean(predicted_class == y))</p>
<h2 id="Learned-Lessons"><a href="#Learned-Lessons" class="headerlink" title="Learned Lessons"></a>Learned Lessons</h2><p>Original code sample is given in the following link.<br><a href="http://cs231n.github.io/neural-networks-case-study/#loss" target="_blank" rel="external">http://cs231n.github.io/neural-networks-case-study/#loss</a></p>
<ol>
<li>Weights are updated layer by layer, same as caffe where learning rate is specified for each weight blob</li>
<li>When initialized with small weights, the loss probability for a given class is np.log(1.0/3), for a 3-class case; This is useful to check if the network runs properly. </li>
<li>step_size is the learning rate, which can be adjusted in each iteration</li>
</ol>
<p>With the above example, you can easily add more hidden layers and change the activation function. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/08/A-toy-example-with-DL/" data-id="ciuzmttii0001ohs66kig6jed" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Softmax-Loss-Derivative" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/08/Softmax-Loss-Derivative/" class="article-date">
  <time datetime="2016-10-08T13:14:19.000Z" itemprop="datePublished">2016-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/08/Softmax-Loss-Derivative/">Softmax-Loss Derivative</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Back propagation is the backbone of the deep learning framework. To understand the principles of BP, I have provided a simple example to compute the derivative of Softmax-loss.</p>
<span>$$\begin{aligned}
L_{i} &amp; = -log(p_{y_{i}}) \\
\end{aligned}$$</span><!-- Has MathJax -->
<span>$$\begin{aligned}
p_{k} &amp; = \frac{e^{f_{k}}}{\sum_{j}{e^{f_{j}}}} \\
\end{aligned}$$</span><!-- Has MathJax -->
<p>First, substitute <span>$p_{y_{i}}$</span><!-- Has MathJax -->  to obtain,<br><span>$$\begin{aligned}
L_{i} &amp; = -log(p_{y_{i}}) \\
        &amp; = -log\frac{e^{f_{y_{i}}}}{\sum_{j}(e^{f_{j}})}\\
        &amp; = - (f_{y_{i}}-log\sum_{j}e^{f_{j}})\\
        &amp; = log\sum_{j}e^{f_{j}}-f_{y_{i}}
\end{aligned}$$</span><!-- Has MathJax --><br>Then compute the derivative of <span>$L_{i}$</span><!-- Has MathJax --> with respect to <span>$f_{k}$</span><!-- Has MathJax -->,<br>\begin{aligned}<br>\frac{\partial L_{i}}{\partial f_k} &amp; = \frac{\partial log \sum_j e^{f_j}}{\partial f_k}-\frac{\partial f_y}{\partial f_k} \\<br> &amp; = \frac{1}{\sum_j e^{f_j}} (\sum_j e^{f_j})’- l(y_i=k) \\<br> &amp; = \frac{e^{f_k}}{\sum_j e^{f_j}}-l (y_i=k) \\<br> &amp; = p_k- l (y_i=k)<br>\end{aligned}<br>It is noted that the <span>$f_{y}$</span><!-- Has MathJax --> should be <span>$f_{y_{i}}$</span><!-- Has MathJax -->, due to problem with hexo math. </p>
<p>The derived equation conforms with the expression in cs231n.<br><a href="http://cs231n.github.io/neural-networks-case-study/#loss" target="_blank" rel="external">http://cs231n.github.io/neural-networks-case-study/#loss</a></p>
<p>However, there actually exists a more complex derivation, which has been well explained in,<br><a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/#disqus_thread" target="_blank" rel="external">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/#disqus_thread</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/08/Softmax-Loss-Derivative/" data-id="ciuzmttjn000aohs62cbqcd4z" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Transfer-Learning-Tips" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/02/Transfer-Learning-Tips/" class="article-date">
  <time datetime="2016-10-02T14:34:52.000Z" itemprop="datePublished">2016-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/02/Transfer-Learning-Tips/">Transfer_Learning_Tips</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>I came across a post which describes the common principles to follow when apply transfer learning or fine-tuning schemes to the CNN. </p>
<p>You can find the original post here.</p>
<p><a href="http://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html" target="_blank" rel="external">http://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html</a></p>
<p>In this post, I quoted some of his statements here. </p>
<h2 id="Transfer-Learning-and-Fine-tuning-DCNNs"><a href="#Transfer-Learning-and-Fine-tuning-DCNNs" class="headerlink" title="Transfer Learning and Fine-tuning DCNNs"></a>Transfer Learning and Fine-tuning DCNNs</h2><p>In practice, we don?t usually train an entire DCNN from scratch with random initialization. This is because it is relatively rare to have a dataset of sufficient size that is required for the depth of network required. Instead, it is common to pre-train a DCNN on a very large dataset and then use the trained DCNN weights either as an initialization or a fixed feature extractor for the task of interest.</p>
<p>Fine-Tuning: Transfer learning strategies depend on various factors, but the two most important ones are the size of the new dataset, and its similarity to the original dataset. Keeping in mind that DCNN features are more generic in early layers and more dataset-specific in later layers, there are four major scenarios:</p>
<p>New dataset is smaller in size and similar in content compared to original dataset: If the data is small, it is not a good idea to fine-tune the DCNN due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the DCNN to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN-features.</p>
<p>New dataset is relatively large in size and similar in content compared to the original dataset: Since we have more data, we can have more confidence that we would not over fit if we were to try to fine-tune through the full network.</p>
<p>New dataset is smaller in size but very different in content compared to the original dataset: Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier from the top of the network, which contains more dataset-specific features. Instead, it might work better to train a classifier from activations somewhere earlier in the network.</p>
<p>New dataset is relatively large in size and very different in content compared to the original dataset: Since the dataset is very large, we may expect that we can afford to train a DCNN from scratch. However, in practice it is very often still beneficial to initialize with weights from a pre-trained model. In this case, we would have enough data and confidence to fine-tune through the entire network.<br>Fine-tuning DCNNs: For this DR prediction problem, we fall under scenario iv. We fine-tune the weights of the pre-trained DCNN by continuing the backpropagation. It is possible to fine-tune all the layers of the DCNN, or it?s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a DCNN contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the DCNN becomes progressively more specific to the details of the classes contained in the DR dataset.</p>
<p>Transfer learning constraints: As we use a pre-trained network, we are slightly constrained in terms of the model architecture. For example, we can?t arbitrarily take out convolutional layers from the pre-trained network. However, due to parameter sharing, we can easily run a pre-trained network on images of different spatial size. This is clearly evident in the case of Convolutional and Pool layers because their forward function is independent of the input volume spatial size. In case of Fully Connected (FC) layers, this still holds true because FC layers can be converted to a Convolutional Layer.</p>
<p>Learning rates: We use a smaller learning rate for DCNN weights that are being fine-tuned under the assumption that the pre-trained DCNN weights are relatively good. We don?t wish to distort them too quickly or too much, so we keep both our learning rate and learning rate decay really small.</p>
<p>Data Augmentation: One of the drawbacks of non-regularized neural networks is that they are extremely flexible: they learn both features and noise equally well, increasing the potential for overfitting. In our model, we apply L2 regularization to avoid overfitting. But even after that, we observed a large gap in model performance on the training and validation DR images, indicating that the fine tuning process is overfitting to the training set. To combat this overfitting, we leverage data augmentation for the DR image dataset.</p>
<p>There are many ways to do data augmentation, such as the popular horizontally flipping, random crops and color jittering. As the color information in these images is very important, we only rotate the images at different angles ? at 0, 90, 180, and 270 degrees.</p>
<h2 id="Other-References"><a href="#Other-References" class="headerlink" title="Other References"></a>Other References</h2><p>Andrej Karpathy@cs231n also has an intuitive explanation on how we should employ transfer learning in practice. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/02/Transfer-Learning-Tips/" data-id="ciuzmttji0009ohs6rrmkwkve" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Deep-Learning-Resources" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/02/Deep-Learning-Resources/" class="article-date">
  <time datetime="2016-10-02T07:18:27.000Z" itemprop="datePublished">2016-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/02/Deep-Learning-Resources/">Deep_Learning_Resources</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>I have collected a bunch of resources that might be helpful to study the field of deep learning. </p>
<h2 id="Courses"><a href="#Courses" class="headerlink" title="Courses"></a>Courses</h2><p>CS231n: Convolutional Neural Networks for Visual Recognition, taught by Andrej Karpathy etc.<br><a href="http://cs231n.stanford.edu" target="_blank" rel="external">http://cs231n.stanford.edu</a></p>
<p>CS224d: Deep Learning for Natural Language Processing<br><a href="http://cs224d.stanford.edu" target="_blank" rel="external">http://cs224d.stanford.edu</a></p>
<p>Machine Learning: 2014-2015, taught by Nando de Freitas<br><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank" rel="external">https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/</a></p>
<p>Machine Learning (fundamental concepts), taught by Andrew Ng<br><a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="external">https://www.coursera.org/learn/neural-networks</a></p>
<p>Neural Network, taught by Geoffrey Hinton<br><a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="external">https://www.coursera.org/learn/neural-networks</a></p>
<h2 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h2><p>Bay Area Deep Learning School, 2016<br><a href="https://www.youtube.com/watch?v=eyovmAtoUx0" target="_blank" rel="external">https://www.youtube.com/watch?v=eyovmAtoUx0</a></p>
<p>Deep Learning Summer School, Montreal 2016<br><a href="http://videolectures.net/deeplearning2016_montreal/" target="_blank" rel="external">http://videolectures.net/deeplearning2016_montreal/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/02/Deep-Learning-Resources/" data-id="ciuzmttiv0004ohs66wfe8yhd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Bay-Area-DeepLearning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/02/Bay-Area-DeepLearning/" class="article-date">
  <time datetime="2016-10-02T03:44:01.000Z" itemprop="datePublished">2016-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/02/Bay-Area-DeepLearning/">Bay_Area_DeepLearning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>I have spent the weekend to watch the video lectures given on Bay Area Deep Learning School and here are some thoughts that found to be particularly interesting, as illustrated below. </p>
<p><img src="img/20161002/dl_practice.png" alt="Deep Learning in Practice"></p>
<p><img src="img/20161002/dl_career.png" alt="Deep Learning Career Path"></p>
<p>Here is the address to watch the video.</p>
<p><a href="https://www.youtube.com/watch?v=eyovmAtoUx0" target="_blank" rel="external">https://www.youtube.com/watch?v=eyovmAtoUx0</a></p>
<p>There is also a very nice article which summarizes the notes made by Andrew Ng.</p>
<p><a href="https://kevinzakka.github.io/2016/09/26/applying-deep-learning/" target="_blank" rel="external">https://kevinzakka.github.io/2016/09/26/applying-deep-learning/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/02/Bay-Area-DeepLearning/" data-id="ciuzmttj20006ohs6dgjasca5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-300w" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/08/16/300w/" class="article-date">
  <time datetime="2016-08-16T08:26:16.000Z" itemprop="datePublished">2016-08-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/08/16/300w/">300w</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>I have trained a 68 point landmark detector with CNN. The input dimension is 40*40 and the enclosing bounding box is generated by the min and max of the landmarks. The network structure still follows the VanillaCNN structure. </p>
<p>Some landmark detection results are shown as below.</p>
<div id="image-table"><br>    <table><br>        <tr><br>            <td style="padding:5px"><br>                <img src="img/20160816/out_0/image_0001.png" height="200" width="250"><br>              </td><br>            <td style="padding:5px"><br>                <img src="img/20160816/out_0/image_0002.png" height="200" width="250"><br>             </td><br>            <td style="padding:5px"><br>                <img src="img/20160816/out_0/image_0008.png" height="200" width="250"><br>             </td><br>             <td style="padding:5px"><br>                <img src="img/20160816/out_0/image_0022.png" height="200" width="250"><br>             </td><br>        </tr><br>    </table><br></div>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/08/16/300w/" data-id="ciuzmttib0000ohs6qt5f94pl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Multitask-CNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/07/19/Multitask-CNN/" class="article-date">
  <time datetime="2016-07-19T15:08:37.000Z" itemprop="datePublished">2016-07-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/07/19/Multitask-CNN/">Multitask_CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Face landmark detection can be combined with other tasks, such as gender classification, pose estimation and so on. This is usually referred as multi-task CNN learning in this context. It is not very difficult to adapt a single-task CNN to a multi-task CNN. We only need to modify the prototxt file and properly define the input and ouput data structures. Here I given an example of multi-task CNN based on VanillaCNN structure.</p>
<p>The network structure is given as follows:</p>
<p><img src="img/20160719/vanilla_multitask.png" alt="VanillaCNN_Multitask"></p>
<p>The dataset is provided as follows:</p>
<p><a href="http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html" target="_blank" rel="external">http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html</a></p>
<p>A recent work, namely “HyperFace”, also provides very detailed analysis on how we can perform multi-task learning with CNN. The presented ideas are very straightforward and can be easily implemented, at least for the multi-task CNN part or the individual parts. </p>
<p><a href="http://arxiv.org/abs/1603.01249" target="_blank" rel="external">http://arxiv.org/abs/1603.01249</a></p>
<p>You can see the HyperFace network structure here. I have briefly copied the figure from the paper. </p>
<p><img src="img/20160719/hyperface.png" alt="HyperFace"></p>
<p>Besides, you can also check the multitask network structure. </p>
<p><img src="img/20160719/multitask_face.png" alt="HyperFace"></p>
<p>As mentioned in the paper, the networks are modified from the Region-based CNN, which is derived from AlexNet. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/07/19/Multitask-CNN/" data-id="ciuzmttiz0005ohs6d9zemdap" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/01/Faster-RCNN-Installation-Guide/">Faster RCNN Installation Guide</a>
          </li>
        
          <li>
            <a href="/2016/10/30/Torch-Installation-Guide/">Torch Installation Guide</a>
          </li>
        
          <li>
            <a href="/2016/10/23/Cross-entropy-Loss-and-Softmax-Classifier/">Cross-entropy Loss and Softmax Classifier</a>
          </li>
        
          <li>
            <a href="/2016/10/08/A-toy-example-with-DL/">A toy example with DL</a>
          </li>
        
          <li>
            <a href="/2016/10/08/Softmax-Loss-Derivative/">Softmax-Loss Derivative</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Cunjian Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>